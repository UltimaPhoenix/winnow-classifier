% DOCUMENT FORMAT %
\documentclass[twocolumn]{article}

% PACKAGE IMPORT %
\usepackage[latin1]{inputenc} %consente di mettere le accentate direttamente
\usepackage[english,italian]{babel} %sillabazione in italiano
\usepackage{cite}
\usepackage{hyperref}
%%pacchetto per l'inclusione di figure eps
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{epstopdf}

\begin{document}
\title{Question Classification and On-Line Alghoritms: Winnow Classifier}
\author{Valerio Barbagallo \& Fabio Grucci}
% Remove command to get current date 
\date{\today}
\maketitle

\begin{abstract}
L'articolo tratta del problema del Question Answering (QA) in particolare della Question Classification (QC).

Nel dettaglio vengono trattati i problemi che sorgono quando si vogliono realizzare algoritmi per la QC: analisi del dominio di interesse, scelta di una tipologia di algoritmo (on-line/off-line), scelta dell'algoritmo, scelta delle feature da applicare, implementazione e confronto con altri classificatori noti.%%non mi piace molto la frase riscrivere

In modo particolare l'interesse è quello di testare l'efficacia di un algoritmo on-line (Winnow) nel dominio della Question Classification, studiandone dettagliatamente le prestazioni sia in relazione alla variazione della grandezza del training set, sia al tipo di feature utilizzate.

%etc~\ldots\ 
\end{abstract}

\section{Introduction}
Con l'aumento della popolarità e della diffusione del web, e conseguentemente con l'aumento 
dell'informazione testuale sul web, il processamento automatico di informazione testuale scritta in 
linguaggio naturale diviene sempre più importante.\cite{Zhalaing}. A tale fine si usano tecniche di 
Information Retrieval, che sono alla base degli attuali motori di ricerca web: quando un utente ha 
un information need, sottomette una query al motore di ricerca, che produrrà
come output un insieme di documenti che con buona probabilità dovrebbero contenere l'informazione 
necessaria all'utente.

\subsection{Question Answering}
Il Question Answering è una variazione dell'information retrieval (IR): 
mentre l'IR è orientata ai documenti, il QA ricerca specifiche informazioni all'interno dei documenti 
cercando di fornire direttamente la risposta all'information need dell'utente~\cite{WeiLi}. Ed è quindi 
un compito più difficile rispetto all'IR, poiché fornire una risposta precisa e concisa è più complesso 
di produrre un intero documento di testo probabilmente molto lungo.
Un sistema di QA è generalmente costituito da 4 moduli:
\begin{description}
	\item[Question Classifier] ha il compito di classificare la domanda in categorie prestabilite;
	\item[Search Engine] basando sulla categoria prodotta dal modulo precedente, ricerca  i documenti che hanno probabilità più alta di contenere la risposta;
	\item[Text Filter] individua le parti di testo all'interno dei documenti, che potrebbero  contenere la risposta;
	\item[Answer Extractor] deduce la risposta dalle parti di testo individuate dal modulo precedente, e le presenta in linguaggio naturale all'utente del sistema.
\end{description}
% disegno

\subsection{Question Classification}
Il processo di QC ha quindi il compito di assegnare particolari categorie alle domande, basandosi sul tipo 
di risposta che la domanda rappresenta. Per classificare le domande, o più in generale del test, 
è necessario prendere in considerazione due aspetti basilari \cite{brown}:
\begin{itemize}
	\item i tipi delle risposte, le categorie;
	\item un insieme di regole di classificazione.
\end{itemize}
 
\subsubsection{Answer Types}
Definire un proprio insieme di categorie da utilizzare nella question classification è una 
delle soluzioni, ma non sempre la migliore. È possibile utilizzare dei sistemi di categorie già usati in precedenza. 
Il riuso di tali sistemi, oltre a far risparmiare una notevole quantità di tempo, è decisamente utile per 
comparare i propri risultati con altri ottenuti in precedenza. 

I primi sistemi di question classification utilizzavano una suddivisione in un piccolo numero 
di categorie: sei o sette. Recentemente i ricercatori si sono interessati nella costruzione 
di categorie migliori: i sistemi attuali solitamente prevedono 
una divisione in 6-7 categorie a grana grossa, e una successiva suddivisione di ogni categoria in altrettante 
di dettaglio. Tra i tipi di risposte più famosi troviamo:
\begin{itemize}
	\item Xin Li e Dan Roth, che propongono una suddivisione in 50 sotto categorie e 6 macro categorie~\cite{XinLi}:	
	\begin{itemize}
		\item Abbreviation;
		\item Entity;
		\item Description;
		\item Human;
		\item Location;
		\item Numerical Value.
	\end{itemize}
	\item Webclopedia, che usa oltre 140 tipi di risposte, anche chiamati qtargets. Questi sono raggruppati in 
	8 macro categorie~\cite{gerber}:	
	\begin{enumerate}
		\item relational qtargets;
		\item abstract qtargets;
		\item semantic qtargets;
		\item syntactic qtargets;
		\item role qtargets;
		\item slot qtargets;
		\item lexical qtargets;
		\item combinations of other qtargets.
	\end{enumerate}
\end{itemize}

\subsubsection{Classification Strategies}
Esistono diverse strategie/regole di classificazione:

\paragraph{Regular expression and hand-written grammar rules} 
sono le prime tecniche utilizzate per la 
question classification, ma hanno dei grossi limiti, sebbene abbiano avuto successo \cite{brown}:
\begin{itemize}
	\item in primo luogo tali tecniche richiedono molto tempo, poiché sono scritte a mano;
	\item in secondo luogo sono poco evolvibili, con il cambiare delle categorie;
	\item se cambiamo l'insieme delle categorie, tutte le regole devono essere riscritte;
	\item infine sono molto difficili da scrivere, soprattutto quando si usano tante categorie a grana fine.
\end{itemize}
Non è un caso, infatti, che tali sistemi utilizzino meno di dieci categorie.

\paragraph{Machine Learning Algorithm}
L'algoritmo più utilizzato in questo campo è SVM, support vector machine. Un'altra architettura 
molto usata è SNoW~\cite{XinLi}. La precision di tali algoritmi, utlizzando solo le parole delle 
domande, si aggira sul 50\%. Ma si possono utilizzare molte altre feature oltre alle 
semplici parole, come ad esempio: le named entity, head chunk etc~\ldots\ 

\paragraph{Language Modeling} 
Un altro metodo, sempre probabilistico come il machine learning, è il language modeling.
Un modello di linguaggio è creato per ogni classe, a partire da tutte le domande 
appartenenti a quella classe. Dato uno di questi modelli, il nostro obiettivo è
scoprire la probabilità con cui la question sia generata da tale modello. Anche in questo caso 
si possono utilizzare altri insiemi di feature oltre alle semplici parole, come le named entity~\cite{pinto}.


\section{The starting point}
Sebbene sia possibile creare classificatori con regole euristiche costruite ad hoc~\cite{DBLP:conf/trec/Voorhees01}, tale approccio richiede un'enorme quantità di tempo e di noioso lavoro. Per di più un approccio troppo specifico sarebbe poco flessibile ai cambiamenti che si trovano nel dominio di interesse.
Al contrario utilizzando tecniche di machine learning è possibile costruire classificatori che abbiano alte performance e che riescano a gestire migliaia di feature. Oltretutto tale approccio è più flessibile e si adatta facilmente a nuove categorie~\cite{Zhang}. Ed è questo l'approccio che intendiamo usare per il nostro classificatore.

Le tecniche di machine learning si distinguono in due grandi categorie:
\begin{enumerate}
	\item Off-Line;
	\item On-Line.
\end{enumerate}
Le principali caratteristiche sulle quali si distinguono le due tipolodie sono: tempo di addestramento, flessibilità e conseguentemente per algoritmo.

Un classificatore off-line deve essere addestrato su un insieme di input (training set). Una volta finita la fase di addestramento, è in grado di classificare, ma non più di imparare. Nel caso si voglia far imparare al classificatore nuovi esempi, l'addestramento deve essere ricominciato. Questo porterebbe ad una notevole perdita di tempo se il dominio di inferenza, ma anche le categorie utilizzate, cambiassero nel tempo (rendendo il training set precedentemente utilizzato per ``costruire'' il classificatore non più coerente con lo stato attuale).

I tempi di training dei classificatori basati su tecniche di machine learning possono essere molto diversi da classificatore a classificatore; tipicamente il tempo di training di un classificatore off-line è maggiore, se confrontato con un classificatore online~\cite{Habib}.

Al contrario un algoritmo on-line impara sul campo e durante l'addestramento può comunque essere interrogato, questo consente ``all'addestratore'' di valutare in continuazione come procede il lavoro di inferenza del dominio da parte del classificatore .

All'apparenza un classificatore on-line offre più vantaggi ma come spesso accade ai ``pro'' vanno sommati dei ``contro'', che in questo caso ricadono sulle prestazioni del classificatore; un classificatore on-line è tipicamente peggiore di un classificatore off-line.


\subsection{On-Line Algorithm}
Adesso che abbiamo una conoscenza generale delle differenze tra le due classi di algoritmi dobbiamo scegliere quale delle due adottare.

Passiamo quindi all'analisi del dominio di interesse, ovvero la question classification (QC); quest'ultima appartiene ad un dominio intrinsecamente variabile, ambiguo e soprattutto dipendente dal tempo.

Il dominio della domande (cioè quello trattato dal question answering e dalla question classification) è variabile nel tempo perché i significati di una parola oltre a essere molteplici possono variare nel tempo, un caso esemplificativo ed evidente è la parola ``laico'' nella nostra lingua, dove fino a pochissimo tempo fa stava ad indicare la non appartenenza all'ecclesia mentre ora ha in pratica preso il significato di ``ateo'' e/o ``agnostico'' (una variazione di significato non indifferente).
Poiché il nostro dominio è fortemente influenzato dalle variazioni del contesto nel quale si trova e poiché l'applicabilità degli algoritmi off-line è stata studiata approfonditamente in molti lavori %inserire riferimento%
mentre non sono stati approfonditi i comportamenti che hanno gli algoritmi on-line sulla Question Classification, la scelta ricade sugli questi ultimi.

\subsection{Winnow}
Scelto di utilizzare un algoritmo on-line bisogna scegliere l'algoritmo di base per poi testarlo ed adattarlo alle nostre esigenze.

Tra gli algoritmi on-line la scelta ricade in particolare sull'algoritmo di Winnow.

L'algoritmo di Winnow è composto da un insieme di ``esperti'' non meglio precisati (che volendo possono essere veri e propri classificatori complessi) ai quali è associato un peso (inizialmente pari a \emph{1}) che vengono interpellati all'arrivo dell'oggetto da classificare.

Ogni ``esperto'' dà come risultato \emph{1} o \emph{0} rispettivamente se l'oggetto appartiene alla classe oppure no.
Vengono sommati i risultati ottenuti dal prodotto del peso di ogni classificatore per la predizione corrispondente (\emph{0} oppure \emph{1}). Se il risultato finale supera la soglia (nel caso semplice pari al numero di esperti) allora l'algoritmo generale risponde in modo positivo, altrimenti in modo negativo.

Se l'algoritmo generale fallisce due sono i casi da distinguere:
\begin{enumerate}
	\item Errore su predizione positiva;
	\item Errore su predizione negativa.
\end{enumerate}
Il peso degli esperti che hanno predetto \emph{1}, viene dimezzato nel primo caso oppure se si ricade nel secondo caso raddoppiato.

L'algoritmo ha molte qualità tra le quali spicca la capacita di non essere influenzato dalla presenza di molti attributi irrilevanti (``winnow è adatto in teoria per problemi con molti attributi irrilevanti''~\cite{Zhang}) e ``winnow impara linearmente con il numero di feature rilevanti, e solo logaritmicamente con il numero totale di feature. questa proprietà sembra cruciale in problemi nei quali il numero di feature potenziali è vasto ma solo poche sono rilevanti''~\cite{snow}.

\section{Winnow Project}
Il nostro classificatore ha la seguente architettura:

\begin{figure}[htb!]
	\centering
	\includegraphics[scale=0.65]{img/Winnow_Classifier_Pipeline.eps}
	\caption{Winnow Classifier pipeline}
\end{figure}
%DISEGNO CON TRE BLOCCHI
%1)	Nome: Parser
%	Input: File TREC
%	Ouput: Domanda
%2) Nome: Filters
%	Input: Domanda
%	Ouput: Vettore/Instance
%1)	Nome: Winnow
%	Input: Vettore/Instance
%	Ouput: Class

Le domande vengono prese dal file TREC \emph{train\_5500.label} disponibile sulla rete, queste vengono ``parsate'' da un modulo che le da in pasto al secondo modulo che si occupa di trasformare le domande, ora in forma testuale, in forma vettoriale per poi farle analizzare al modulo che poi si occupa di classificare.

\subsection{Obiettivi}
Entriamo nella fase realizzativa, la piattaforma scelta è quella di WEKA, essa ci offre la possibilità di utilizzare e poter paragonare agilmente l'efficacia del classificatore con gli altri già implementati nello strumento.
Nel dettaglio gli obiettivi sono:
\begin{itemize}
	\item Creare un classificatore che rispetti l'interfaccia di WEKA;
	\item Paragonare il classificatore con algoritmi off-line già implementati in WEKA;
	\item Analizzare delle prestazioni relativamente a ciascuna feature introdotta;
	\item Studiare l'andamento del classificatore all'aumento dell'insieme di training.
\end{itemize}

\section{Pre-Processing - Feature Extraction}
Per un sistema di classificazione la scelta delle feature da applicare è un punto delicato, per tale motivo bisogna selezionarle in modo ponderato e compatibile col dominio che viene trattato dal classificatore.

Il dominio trattato dal classificatore è composto da domande, le quali per loro propria natura sono ``compattate'' (brevi/compresse) in una certa finestra di lunghezza comunque molto piccola (in media).

L'obiettivo del preprocessing è quello di arricchire le informazioni così da averne poi un beneficio diretto sul processo di classificazione immediatamente successivo.

Le feature che si vogliono introdurre hanno lo scopo di prendere in considerazione:
\begin{enumerate}
	\item L'ordine delle parole;
	\item Il valore semantico delle parole.
\end{enumerate}
La tecnica che si è adottata per soddisfare questi obiettivi è stata quella di aggiungere alla domanda nuovi elementi, ad esempio quelli ottenuti dalla giustapposizione delle parole presenti nelle domanda stessa.
\begin{figure}[htb!]
	\centering
	\includegraphics[scale=0.85]{img/Filters_Detail.eps}
	\caption{Filters example}
\end{figure}
Ad esempio se una domanda è nella forma ``What is the fire'', vengono aggiunte le parole ``What\_is'', ``is\_the'', ``the\_fire'', ottenendo quindi un risultato del genere: ``What is the fire What\_is is\_the the\_fire''.
È semplice vedere come soprattutto la prima composizione (i.e. ``What\_is'') sia di notevolmente importante nella categorizzazione delle domande.

Con la tecnica sopra descritta sono state aggiunte le seguenti feature:
\begin{enumerate}
	\item Relazioni tra le parole sia a coppie che a triple;
	\item Parole ``stemmate'' (lasciando anche quelle non originali), nel dettaglio tramite lo ``Snowball Stemmer'' ed un altro rudimentale sviluppato da noi;
	\item Relazione tra la prima parola della domanda e l'ultima;
	\item Utilizzo di WordNet per la derivazione del tipo sintattico della parola ed in più la giustapposizione dei tipi sintattici per coppie e per triple. % Referenza su wordnet %
\end{enumerate}

L'ultimo filtro trasforma la domanda nel vettore delle occorrenze seguendo il modello \emph{Bag of words}.

\section{Experimental Set-Up}

\subsection{Weka Interface}
Si è scelto di sviluppare un classificatore che rispettasse l'interfaccia di \emph{WEKA} (v 3.5.7), per poter poi così comparare il lavoro da noi svolto con altri classificatori già implementati all'interno dello strumento.

\subsection{Filters}
Per aggiungere feature alla domanda sono stati utilizzati:
\begin{itemize}
	\item SnowballStemmer - la cui interfaccia è esposta da WEKA;
	\item WordNet 3 - per la derivazione dei sinonimi;
	\item Più quelli da noi creati (illustrati nella sezione precedente).
\end{itemize}

%%usato weka implementato da usare
%%snowball stemmer, wordnet 3

\subsection{Report}
Per avere una visione più intuitiva delle prestazioni del classificatore sono stati creati dei report grafici creati con \emph{JFreechart 1.0.9}.

Tale strumento consente per l'appunto di poter creare dei report riassuntivi e comparativi partendo da dati numerici.

\subsection{Winnow Classifier The Algorithm}
%Approccio Bag of Word
L'algoritmo di Winnow realizzato ha la seguente architettura: ad ogni ``classe'' è associata una collezione di ``esperti'', ognuno dei quali associato ad una parola, ovvero un esperto viene interpellato ogni qual volta la parola associata ad esso è presente nella domanda.
L'algoritmo quindi ha come input una domanda, per ogni parola presente nella domanda interroga il relativo esperto, se la somma dei punteggi di tutti gli esperti supera una certa soglia allora la predizione per quella determinata classe è positiva altrimenti no.
In pratica ogni esperto quindi associa un punteggio (o peso) alla parola al quale è relativo, tale punteggio viene gestito secondo l'algoritmo classico, cioè incrementandolo (raddoppiando il peso) in caso di errore su predizione negativa o decrementandolo (dimezzando il peso) in caso di errore su predizione positiva.


\subsection{SMO Comparison}
Tra i classificatori off-line con cui effettuare la comparazione è stato scelto il classificatore SMO, integrato in WEKA.

Per poter effettuare la comparazione con un classificatore di tipo profondamente diverso (SMO è off-line al contrario del nostro) è stato necessario addestrare diverse volte il classificatore, nel dettaglio ad ogni cambiamento dell'insieme di training.

\subsection{Hardware}
La macchina di test ha le seguenti caratteristiche principali:
\begin{itemize}
	\item Processore: Intel Core 2 Duo T7500; %Oppure il tuo
	\item RAM: 3GB;
	\item S.O.: Windows Vista Business x64 SP1.
\end{itemize}

\section{Experimental Result}
Analizziamo ora i risultati ottenuti. Ci concentriamo dapprima sul nostro classificatore, dopodichè sul confronto con SMO.

\subsection{Winnow Result}
In questo paragrafo analizziamo, quindi, alcuni risultati ottenuti dal nostro classificatore a seconda delle feature utilizzate.

\subsubsection{All Feature (best result)}
Il miglior risultato ottenuto dal nostro classificatore, utilizzando tutte le feature descritte nei paragrafi precedenti (dalle parole stemmate all'ordine delle parole nella domanda), ha portato ad un'accuracy dell'80.6\%. Riportiamo di seguito uno schema riassuntivo del risultato ottenuto nella stessa forma che prevede l'"Evaluator" di Weka:
{\footnotesize
\begin{verbatim}
Correctly Classified Instances    403         80.6    %
Incorrectly Classified Instances   71         14.2    %
Kappa statistic                     0.8098
Mean absolute error                 0.065 
Root mean squared error             0.255 
Relative absolute error            25.9565 %
Root relative squared error        72.1325 %
UnClassified Instances             26         5.2    %
Total Number of Instances         500   
\end{verbatim}
}

Volendo però analizzare più nel dettaglio tale risultato, ci accorgiamo come il classificatore non è performante alla stessa maniera in tutte le categorie. Non capita mai che il sistema classifichi erroneamente una domanda appartentente alla macro categoria abbreviation. Mentre esattamente opposto è il comportamento relativo alla categoria entity, che è spesso causa di errore.
{\footnotesize
\begin{verbatim}
   a   b   c   d   e   f   <-- classified as
   7   2   0   0   0   0 |   a = ABBR
   0 129   2   0   0   6 |   b = DESC
   0  10  58   0   4   7 |   c = ENTY
   0   0   3  56   2   0 |   d = HUM
   0   2  13   0  59   3 |   e = LOC
   0   4   6   5   2  94 |   f = NUM
\end{verbatim}
}

Ma un analisi del genere è ancora sommaria: la precision non è l'unico parametro con cui valutare le prestazioni di un classificatore. Nello studio dei classificatori i parametri più utilizzati sono infatti la precision e la recall. Il primo si riferisce al rapporto tra il numero di domande classificate correttamente per una certa categoria diviso il numero di domande classificate in totale sempre appartenenti alla stessa categoria. Il secondo, invece, corrisponde al rapporto tra il numero di domande classificate correttamente per una categoria diviso tutte le domande appartenenti a tale categoria.

Difatti, analizzando nuovamente i dati, possiamo notare come per la categoria abbreviation la precision è 1, ma non la recall. Mentre per la classe entity sia la precision che la recall sono tra i valori più bassi fatti registrare dal nostro classificatore. La tabella \ref{best} riporta i dati tecnici di cui stiamo parlando, più altri che sicuramente saranno noti agli esperti del settore e di Weka.

\begin{table}[htbp]
	\centering
	{\footnotesize
		\begin{tabular}{|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}| }		
		 \hline		 
	TP Rate & FP Rate &  Precision  & Recall &F-Measure &  ROC Area &  Class \\ \hline
  0.778 &   0     &     1      &   0.778  &   0.875   &   0.889  &  ABBR \\ \hline 
  0.942   &  0.053   &   0.878  &   0.942  &   0.908   &   0.934  &  DESC \\ \hline
  0.734   &  0.061  &    0.707  &   0.734  &   0.72    &   0.749  &  ENTY \\ \hline 
  0.918 &    0.012  &    0.918  &   0.918  &   0.918   &   0.925  &  HUM \\ \hline 
  0.766  &   0.02   &    0.881  &   0.766   &  0.819   &   0.893  &  LOC \\ \hline
  0.847   &  0.044  &    0.855  &   0.847  &   0.851   &   0.947  &  NUM \\ \hline  
		\end{tabular}
		}
		\caption{Detailed Accuracy By Class}
		\label{best} 
\end{table}

Infine, come ci eravamo proposti, analizziamo anche l'andamento delle prestazioni del classificatore al variare dell'insieme di input, questo per capire come si comporta l'algoritmo, che, ripetiamo, essendo on-line, è in grado di imparare anche durante l'utilizzo dello stesso. Si può osservare tale andamento nella figura \ref{andamento_best}. 

\begin{figure}[htp!]
 \begin{center}
	\includegraphics[scale=0.22]{winnow_all_feature/Winnow_All_Feature.png}
	\end{center}
	\caption{Andamento di winnow con tutte le feature}
	\label{andamento_best}
\end{figure}

Si noti come il comportamento di winnow sia piuttosto oscillatorio e come, nonostante una breve fase iniziale di assestamento, il comportamento di winnow, seppur restando oscillatorio, si stabilizza intorno a certi valori, senza mostrare un incremento notevole all'aumentare dell'input. Tale fenomeno ovviamente potrebbe dipendere sia dal training set, che sebbene comprende 5500 domande, potrebbe ritenersi ancora piuttosto ridotto; sia dalla scelta delle feature utilizzate; ma anche dai parametri/pesi propri dell'algoritmo che, con un'accurata fase di tuning, potrebbero portare a dei miglioramenti sensibili delle performance.

\subsubsection{Zero feature}
Mostriamo ora, invece, i risultati che winnow otteneva prima di applicare qualsiasi feature da noi proposta. Ossia in questo esempio viene utilizzato semplicemente l'approccio bag of word, dove ogni esperto è una delle parole che occorrono nella domanda, senza andare a fare nessuna modifica di quest'ultima. Si può notare come l'accuracy sia intorno al 50\% e quindi come l'utilizzo delle feature da noi studiate, abbia portato ad un miglioramento del 30\%.
{\footnotesize
\begin{verbatim}
Correctly Classified Instances    253        50.6    %
Incorrectly Classified Instances  101        20.2    %
Kappa statistic                     0.6453
Mean absolute error                 0.0927
Root mean squared error             0.3045
Relative absolute error            48.7635 %
Root relative squared error        98.0838 %
UnClassified Instances            146        29.2    %
Total Number of Instances         500     
\end{verbatim}
}

Dalla tabella \ref{zero} si può notare come la classe che in queste condizioni ha il risultato peggiore, sia in termini di precision che di recall, è la classe description, che è quindi quella che trae maggior beneficio dalle feature da noi utilizzate.
\begin{table}[htbp]
	\centering
	{\footnotesize
		\begin{tabular}{|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}| }		
		 \hline		 
	TP Rate & FP Rate &  Precision  & Recall &F-Measure &  ROC Area &  Class \\ \hline
  0.875   &  0.009  &    0.7     &  0.875 &    0.778   &   0.886  &  ABBR \\ \hline  
  0.585   &  0.083  &    0.554   &  0.585 &    0.569   &   0.581  &  DESC \\ \hline  
  0.701   &  0.167  &    0.495   &  0.701 &    0.58    &   0.71   &  ENTY \\ \hline  
  0.8     &  0.027  &    0.846   &  0.8   &    0.822   &   0.871  &  HUM \\ \hline  
  0.75    &  0.049  &    0.785   &  0.75  &    0.767   &   0.833  &  LOC \\ \hline  
  0.709   &  0.012  &    0.961   &  0.709 &    0.816   &   0.91   &  NUM \\ \hline  
		\end{tabular}
		}
		\caption{Zero feature result}
		\label{zero} 
\end{table}

\subsubsection{Stemmer only}
Un altro risultato degno di nota, è quello prodotto dall'utilizzo delle sole parole stemmate. Quest'unica feature da sola apporta un miglioramento del 15\% rispetto al caso di nessuna feature utilizzata. Riteniamo quindi tale caratteristica di estrema importanza nel dominio della question classification.
{\footnotesize
\begin{verbatim}
Correctly Classified Instances    325         65      %
Incorrectly Classified Instances  141         28.2    %
Kappa statistic                     0.6123
Mean absolute error                 0.128 
Root mean squared error             0.3578
Relative absolute error            51.9759 %
Root relative squared error       102.0796 %
UnClassified Instances             34          6.8    %
Total Number of Instances          500 
\end{verbatim}
}

\subsection{Winnow vs SMO}
In questo paragrafo compariamo invece il risultato prodotto da winnow, un algoritmo on-line, con SMO, un'implementazione particolare delle Support Vector Machine, ossia un algoritmo off-line.

\subsubsection{All Feature}
In questo test sono state utilizzate tutte le feature da noi analizzate: notiamo nella figura \ref{comparison1} come SMO abbia prestazioni superiori rispetto a Winnow, ad esclusione di una fase transitoria iniziale. Ma già dopo un training set composto da 1000 domande, SMO è decisamente supeirore a Winnow. Oltretutto SMO mostra una migliore tendenza a crescere in prestazione con l'aumentare dell'insieme di training, fino ad arrivare a circa il 90\% di accuracy, mentre Winnow resta piuttosto costante. Però è doveroso dire che SMO, a differenza di Winnow, oltre a essere molto più lento nella fase di addestramento, non è più in grado di imparare dopo che tale fase sia terminata.

\begin{figure}[htp!]
 \begin{center}
	\includegraphics[scale=0.22]{comparison/ClassifierComparison2.png}
	\end{center}
	\caption{Winnow vs SMO with all feature}
	\label{comparison1}
\end{figure}

\subsubsection{Zero Feature}
In questo secondo test, rappresentato dalla figura \ref{comparison2}, non viene utilizzata alcuna feature, ma viene semplicemente data in pasto al classificatore la domanda così com'è. Ciò che risulta evidente è come il compostamento di SMO sia molto simile a quello del caso precedente (con tutte le feature), mentre Winnow mostra delle serie limitazioni qualora gli esperti utilizzati non sia poi così esperti.
\begin{figure}[htp!]
 \begin{center}
	\includegraphics[scale=0.22]{comparison/ClassifierComparison3.png}
	\end{center}
	\caption{Winnow vs SMO with zero feature}
	\label{comparison2}
\end{figure}

\section{Conclusion}
%BOZZA CONCETTI%
In conclusione si può dire che l'algoritmo di Winnow non tradisce le aspettative, pur avendo prestazioni nettamente inferiori ad un classificatore off-line (ossia SMO).

Un'interessante caratteristica che si può notare di Winnow è come il suo comportamento degradi molto poco con l'aumentare del numero di esperti/feature utilizzate, mentre anche solo pochi esperti rilevanti apportano un grande contributo alle performance del classificatore. Questo va a conferma di quanto già è stato notato da \cite{Zhang} e da \cite{snow}. 

A tal proposito, le feature che hanno contribuito maggiormente all'incremento delle prestazioni sono lo stemmer e la coppia formata dalle prime due parole di ogni domanda.

Abbiamo, poi, osservato come la classe entity sia quella di più difficile classificazione, e che quindi richiederebbe l'utilizzo di feature adatte e specifiche allo scopo, come le tecniche di named entity recognition, con l'uso di opportuni tag da inserire/sostituire all'interno della domanda.

Un'altra interessante osservazione, come visibile dai grafici, è il comportamento oscillatorio di Winnow, caratteristica propria dell'algoritmo. L'ampiezza dell'oscillazione dipende dai parametri interni dell'algoritmo.

Considerando che le feature introdotte sono semplici e che il tuning non è stato fatto in modo minuzioso, i margini di miglioramento di certo ci sono.


\bibliography{biblio}
\bibliographystyle{plain}

\end{document}